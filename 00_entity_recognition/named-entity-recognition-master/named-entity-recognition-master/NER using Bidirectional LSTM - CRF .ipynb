{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "In Natural Language Processing (NLP) an Entity Recognition is one of the common problem. The entity is referred to as the part of the text that is interested in. In NLP, NER is a method of extracting the relevant information from a large corpus and classifying those entities into predefined categories such as location, organization, name and so on. \n",
    "Information about lables: \n",
    "* geo = Geographical Entity\n",
    "* org = Organization\n",
    "* per = Person\n",
    "* gpe = Geopolitical Entity\n",
    "* tim = Time indicator\n",
    "* art = Artifact\n",
    "* eve = Event\n",
    "* nat = Natural Phenomenon\n",
    "\n",
    "        1. Total Words Count = 1354149 \n",
    "        2. Target Data Column: Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.6/43.6 kB 1.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\charl\\e2_tata\\tata_env\\lib\\site-packages (from seqeval) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\charl\\e2_tata\\tata_env\\lib\\site-packages (from seqeval) (1.3.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\charl\\e2_tata\\tata_env\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\charl\\e2_tata\\tata_env\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\charl\\e2_tata\\tata_env\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16207 sha256=ec6457e27e49e6f2d5217bea1d3f6a172691e9ea5d9c63c064f97c3e24cf5035\n",
      "  Stored in directory: c:\\users\\charl\\appdata\\local\\pip\\cache\\wheels\\e2\\a5\\92\\2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wt4u0Lf1YJPH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ON est LA!!! ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmFVrk0JY-Mv"
   },
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "df = pd.read_csv('ner_dataset.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1560703170937,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "LYlRwss8ZPZr",
    "outputId": "c3366c42-0a14-4925-c108-af64ba6d1921"
   },
   "outputs": [],
   "source": [
    "#Display first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations : \n",
    "* There are total 47959 sentences in the dataset.\n",
    "* Number unique words in the dataset are 35178.\n",
    "* Total 17 lables (Tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the unique Tags\n",
    "df['Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking null values, if any.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of missing values in 'Sentence #' attribute. So we will use pandas fillna technique and use 'ffill' method which propagates last valid observation forward to next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PTsjCdBZ9Xy"
   },
   "outputs": [],
   "source": [
    "df = df.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9DYzRvMagm5"
   },
   "outputs": [],
   "source": [
    "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['POS'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying one full sentence\n",
    "getter = sentence(df)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence with its pos and tag.\n",
    "sent = getter.get_text()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the sentences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3F0_tiOmaiVi"
   },
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the parameters for LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRQJJSoyamU4"
   },
   "outputs": [],
   "source": [
    "# Number of data points passed in each iteration\n",
    "batch_size = 64 \n",
    "# Passes through entire dataset\n",
    "epochs = 8\n",
    "# Maximum length of review\n",
    "max_len = 75 \n",
    "# Dimension of embedding vector\n",
    "embedding = 40 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Data\n",
    "We will process our text data before feeding to the network.\n",
    "* Here word_to_index dictionary used to convert word into index value and tag_to_index is for the labels. So overall we represent each word as integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32qpbWMVau_5"
   },
   "outputs": [],
   "source": [
    "#Getting unique words and labels from data\n",
    "words = list(df['Word'].unique())\n",
    "tags = list(df['Tag'].unique())\n",
    "# Dictionary word:index pair\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
    "word_to_index[\"UNK\"] = 1\n",
    "word_to_index[\"PAD\"] = 0\n",
    "\n",
    "# Dictionary lable:index pair\n",
    "# label is key and value is index.\n",
    "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
    "tag_to_index[\"PAD\"] = 0\n",
    "\n",
    "idx2word = {i: w for w, i in word_to_index.items()}\n",
    "idx2tag = {i: w for w, i in tag_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The word India is identified by the index: {}\".format(word_to_index[\"India\"]))\n",
    "print(\"The label B-org for the organization is identified by the index: {}\".format(tag_to_index[\"B-org\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcC_UuUbav7y"
   },
   "outputs": [],
   "source": [
    "# Converting each sentence into list of index from list of tokens\n",
    "X = [[word_to_index[w[0]] for w in s] for s in sentences]\n",
    "\n",
    "# Padding each sequence to have same length  of each word\n",
    "X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-C7iFNjaytc"
   },
   "outputs": [],
   "source": [
    "# Convert label to index\n",
    "y = [[tag_to_index[w[2]] for w in s] for s in sentences]\n",
    "\n",
    "# padding\n",
    "y = pad_sequences(maxlen = max_len, sequences = y, padding = \"post\", value = tag_to_index[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbnAi9kwa0gL"
   },
   "outputs": [],
   "source": [
    "num_tag = df['Tag'].nunique()\n",
    "# One hot encoded labels\n",
    "y = [to_categorical(i, num_classes = num_tag + 1) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmj_9AzCa23d"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training input data : \", X_train.shape)\n",
    "print(\"Size of training output data : \", np.array(y_train).shape)\n",
    "print(\"Size of testing input data : \", X_test.shape)\n",
    "print(\"Size of testing output data : \", np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the first sentence before and after processing.\n",
    "print('*****Before Processing first sentence : *****\\n', ' '.join([w[0] for w in sentences[0]]))\n",
    "print('*****After Processing first sentence : *****\\n ', X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First label before and after processing.\n",
    "print('*****Before Processing first sentence : *****\\n', ' '.join([w[2] for w in sentences[0]]))\n",
    "print('*****After Processing first sentence : *****\\n ', y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional LSTM-CRF Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1560703209499,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "6WRJfQ5ca4vD",
    "outputId": "a908468a-3b1f-4680-afe7-6cc22a8a9394"
   },
   "outputs": [],
   "source": [
    "num_tags = df['Tag'].nunique()\n",
    "# Model architecture\n",
    "input = Input(shape = (max_len,))\n",
    "model = Embedding(input_dim = len(words) + 2, output_dim = embedding, input_length = max_len, mask_zero = True)(input)\n",
    "model = Bidirectional(LSTM(units = 50, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)\n",
    "crf = CRF(num_tags+1)  # CRF layer\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Checkpoint each epoch to check and save the best model performance till last and also avoiding further validation loss drop due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJcJLVXWa7r1"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath = 'model.h5',\n",
    "                       verbose = 0,\n",
    "                       mode = 'auto',\n",
    "                       save_best_only = True,\n",
    "                       monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2003225,
     "status": "ok",
     "timestamp": 1560708147077,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "SjKhhXHMG-jJ",
    "outputId": "bd461b08-3920-4920-c3a6-10eb8f3cf432"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, np.array(y_train), batch_size=batch_size, epochs=epochs,\n",
    "                    validation_split=0.1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1054,
     "status": "ok",
     "timestamp": 1560705868376,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "vEsREje5ubq-",
    "outputId": "083b774d-ae35-4720-e515-7930617364d4"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1560709905938,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "QElZwYqqbSFV",
    "outputId": "60eea78e-8ea6-45be-9151-281264768c72"
   },
   "outputs": [],
   "source": [
    "acc = history.history['crf_viterbi_accuracy']\n",
    "val_acc = history.history['val_crf_viterbi_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(figsize = (8, 8))\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1215,
     "status": "ok",
     "timestamp": 1560709916909,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "sASoBK0_bYgo",
    "outputId": "bfe6b0a3-a22b-4a0a-db9c-793a71184516"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNtfnN3kdiDr"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_test_true = np.argmax(y_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlAMoN7Lfx4k"
   },
   "outputs": [],
   "source": [
    "# Convert the index to tag\n",
    "y_pred = [[idx2tag[i] for i in row] for row in y_pred]\n",
    "y_test_true = [[idx2tag[i] for i in row] for row in y_test_true] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2597,
     "status": "ok",
     "timestamp": 1560709997195,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "IXWG7vqDf7X4",
    "outputId": "8fe95519-f489-4a35-aabb-322dda91d765"
   },
   "outputs": [],
   "source": [
    "print(\"F1-score is : {:.1%}\".format(f1_score(y_test_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5255,
     "status": "ok",
     "timestamp": 1560710007669,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "8E2X4JzEgJjK",
    "outputId": "e963aa51-efe0-4523-952a-d2978ca48290"
   },
   "outputs": [],
   "source": [
    "report = flat_classification_report(y_pred=y_pred, y_true=y_test_true)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1560710040586,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "hdmpuybYxPJ3",
    "outputId": "57b72e90-9032-4573-f7b5-928a517c6ee0"
   },
   "outputs": [],
   "source": [
    "# At every execution model picks some random test sample from test set.\n",
    "i = np.random.randint(0,X_test.shape[0]) # choose a random number between 0 and len(X_te)b\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_test[i], -1)\n",
    "\n",
    "print(\"Sample number {} of {} (Test Set)\".format(i, X_test.shape[0]))\n",
    "# Visualization\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_test[i], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-2], idx2tag[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results looks quite interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_index.pickle', 'wb') as f:\n",
    "    pickle.dump(word_to_index, f)\n",
    "\n",
    "with open('tag_to_index.pickle', 'wb') as f:\n",
    "    pickle.dump(tag_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tata_env",
   "language": "python",
   "name": "tata_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
